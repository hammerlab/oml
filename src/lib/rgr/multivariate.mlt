
(*
   Copyright 2015:
     Leonid Rozenberg <leonidr@gmail.com>

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
*)

open Test_utils

open Util

module Descriptive = Statistics.Descriptive
module Vectors = Uncategorized.Vectors

let looe_manually lambda pred resp =
  let predi = Array.to_list pred |> List.mapi (fun i p -> (i, p)) in
  let respi = Array.to_list resp |> List.mapi (fun i r -> (i, r)) in
  let without i =
    List.filter (fun (j, _) -> j <> i) predi |> Array.of_list |> Array.map snd,
    List.filter (fun (j, _) -> j <> i) respi |> Array.of_list |> Array.map snd
  in
  pred
  |> Array.mapi (fun i p ->
    let p_pred, p_resp = without i in
    let opt   = opt ~add_constant_column:false ~l2_regularizer:(`S lambda) () in
    let model = regress ~opt p_pred ~resp:p_resp in
    resp.(i) -. eval model p)

(* Testing the accuracy of numerical algorithms is hard (and fun).
  Linear Regression provides an interesting example.
  There are two parameters that we use to tune the range of data to be
  generated by the tests. They give the user a gauge on the accuracy of
  the algorithms. *)
let () =
  let add_random_test
    ?title ?nb_runs ?nb_tries ?classifier
    ?reducer ?reduce_depth ?reduce_smaller ?random_src gen f spec =
    Test.add_random_test_group "Regression"
      ?title ?nb_runs ?nb_tries ?classifier
      ?reducer ?reduce_depth ?reduce_smaller ?random_src gen f spec
  in
  let max_samples = 10 in
  let max_predictors = 3 in
  add_random_test
    ~title:"General can recover coefficients."
    Gen.(general_model 1e11 ~max_samples ~max_predictors)
    (fun (pred, coef, resp) ->
      let glm = regress ~opt:(opt ~add_constant_column:false ()) ~resp pred in
      Vectors.equal ~d:1e-2 (coefficients glm) coef)
    Spec.([just_postcond_pred is_true]);

  add_random_test
    ~title:"Residuals are just the difference between eval and resp"
    Gen.(general_model 1e5 ~max_samples ~max_predictors)
    (fun (pred, _, resp) ->
      let glm = regress ~resp pred in
      let r0 = (residuals glm).(0) in
      let e0 = resp.(0) -. eval glm pred.(0) in
      (*Printf.printf "%.20f\t%.20f\t%b\t%b\n" r0 e0 (r0 = e0) (equal_floats ~d:dx r0 e0)*)
      equal_floats ~d:1e-6 r0 e0)
    Spec.([just_postcond_pred is_true]);

  add_random_test
    ~title:"Ridge coefficients are smaller."
    Gen.(general_model 1e11 ~max_samples ~max_predictors)
    (fun (pred, _, resp) ->
      let glm = regress ~resp pred in
      let m = Descriptive.mean resp in
      let lmd = m *. m in    (* has to be big enough *)
      let opt = opt ~add_constant_column:false ~l2_regularizer:(`S lmd) () in
      let rdg = regress ~opt ~resp pred in
      let rd = Vectors.dot (coefficients rdg) (coefficients rdg) in
      let gd = Vectors.dot (coefficients glm) (coefficients glm) in
      rd < gd)
    Spec.([just_postcond_pred is_true]);

  (* Computing LOOE has a subtraction of 1/S^2 - 1/lambda so it can be
     very imprecise when numbers get big. *)
  add_random_test
    ~title:"Leave-One-Out-Error is same as manually."
    Gen.(general_model 1e7 ~max_samples:5 ~max_predictors:3
         |> zip2 (bpos_float 1e7))
    (fun (lambda, (pred, _, resp)) ->
      let open Lacaml.D in
      let manually = looe_manually lambda pred resp in
      let svdally  = Vec.to_array (Svd.looe (Svd.svd (Mat.of_array pred)) (Vec.of_array resp) lambda) in
      Vectors.equal ~d:1e9 manually svdally)
    Spec.([just_postcond_pred is_true]);

  ()
